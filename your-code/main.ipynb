{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before your start:\n",
    "- Read the README.md file\n",
    "- Comment as much as you can and use the resources in the README.md file\n",
    "- Happy learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hyperoptNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "     ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.0/1.6 MB 653.6 kB/s eta 0:00:03\n",
      "     --- ------------------------------------ 0.1/1.6 MB 1.4 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 0.3/1.6 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 0.4/1.6 MB 2.2 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 0.6/1.6 MB 2.4 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 0.7/1.6 MB 2.5 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 0.8/1.6 MB 2.6 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 1.0/1.6 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 1.1/1.6 MB 2.6 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 1.3/1.6 MB 2.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 1.4/1.6 MB 2.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.6/1.6 MB 2.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.6/1.6 MB 2.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\blanx\\anaconda3\\envs\\clase\\lib\\site-packages (from hyperopt) (1.26.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\blanx\\anaconda3\\envs\\clase\\lib\\site-packages (from hyperopt) (1.11.4)\n",
      "Requirement already satisfied: six in c:\\users\\blanx\\anaconda3\\envs\\clase\\lib\\site-packages (from hyperopt) (1.16.0)\n",
      "Collecting networkx>=2.2 (from hyperopt)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting future (from hyperopt)\n",
      "  Downloading future-0.18.3.tar.gz (840 kB)\n",
      "     ---------------------------------------- 0.0/840.9 kB ? eta -:--:--\n",
      "     ---- ---------------------------------- 92.2/840.9 kB 2.6 MB/s eta 0:00:01\n",
      "     ---------- --------------------------- 235.5/840.9 kB 2.9 MB/s eta 0:00:01\n",
      "     ----------------- -------------------- 389.1/840.9 kB 3.0 MB/s eta 0:00:01\n",
      "     ----------------------- -------------- 522.2/840.9 kB 3.0 MB/s eta 0:00:01\n",
      "     ------------------------------ ------- 665.6/840.9 kB 3.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ - 809.0/840.9 kB 3.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 840.9/840.9 kB 3.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: tqdm in c:\\users\\blanx\\anaconda3\\envs\\clase\\lib\\site-packages (from hyperopt) (4.66.1)\n",
      "Collecting cloudpickle (from hyperopt)\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting py4j (from hyperopt)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "     ---------------------------------------- 0.0/200.5 kB ? eta -:--:--\n",
      "     ----------------------- -------------- 122.9/200.5 kB 3.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 200.5/200.5 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\blanx\\anaconda3\\envs\\clase\\lib\\site-packages (from tqdm->hyperopt) (0.4.6)\n",
      "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.2/1.6 MB 3.1 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.3/1.6 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.4/1.6 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.6/1.6 MB 2.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.7/1.6 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.8/1.6 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.0/1.6 MB 3.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.1/1.6 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.3/1.6 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.4/1.6 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.6/1.6 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 3.0 MB/s eta 0:00:00\n",
      "Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Building wheels for collected packages: future\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492054 sha256=bb2f9c7ad303abfb8b4d070dbfb8f6ea96a0a6adec8f2560ccadc09ddf9866e6\n",
      "  Stored in directory: c:\\users\\blanx\\appdata\\local\\pip\\cache\\wheels\\5e\\a9\\47\\f118e66afd12240e4662752cc22cefae5d97275623aa8ef57d\n",
      "Successfully built future\n",
      "Installing collected packages: py4j, networkx, future, cloudpickle, hyperopt\n",
      "Successfully installed cloudpickle-3.0.0 future-0.18.3 hyperopt-0.2.7 networkx-3.2.1 py4j-0.10.9.7\n"
     ]
    }
   ],
   "source": [
    "#%pip install scikit-plot\n",
    "#%pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import scikitplot as skplt\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR    # el modelo , el alias es cosa mia\n",
    "\n",
    "from sklearn.model_selection import train_test_split as tts  # el alias es cosa mia\n",
    "\n",
    "from sklearn.datasets import load_diabetes   # dataset\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mse    # alias mio\n",
    "\n",
    "from sklearn.metrics import mean_squared_log_error as msle # con logaritmos\n",
    "\n",
    "from sklearn.metrics import r2_score as r2\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression as LogReg\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "\n",
    "from sklearn.metrics import precision_score as prec\n",
    "\n",
    "from sklearn.metrics import recall_score as rec\n",
    "\n",
    "from sklearn.metrics import f1_score as f1\n",
    "\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "\n",
    "from sklearn.metrics import roc_curve as roc\n",
    "\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score as kappa\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.model_selection import GridSearchCV  \n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV  # este es random\n",
    "\n",
    "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this lab, we will use two datasets. Both datasets contain variables that describe apps from the Google Play Store. We will use our knowledge in feature extraction to process these datasets and prepare them for the use of a ML algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1 - Loading and Extracting Features from the First Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this challenge, our goals are: \n",
    "\n",
    "* Exploring the dataset.\n",
    "* Identify the columns with missing values.\n",
    "* Either replacing the missing values in each column or drop the columns.\n",
    "* Conver each column to the appropriate type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first dataset contains different information describing the apps. \n",
    "\n",
    "Load the dataset into the variable `google_play` in the cell below. The dataset is in the file `googleplaystore.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine all variables and their types in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since this dataset only contains one numeric column, let's skip the `describe()` function and look at the first 5 rows using the `head()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that there are a few columns that could be coerced to numeric.\n",
    "\n",
    "Start with the reviews column. We can evaluate what value is causing this column to be of object type finding the non-numeric values in this column. To do this, we recall the `to_numeric()` function. With this function, we are able to coerce all non-numeric data to null. We can then use the `isnull()` function to subset our dataframe using the True/False column that this function generates.\n",
    "\n",
    "In the cell below, transform the Reviews column to numeric and assign this new column to the variable `Reviews_numeric`. Make sure to coerce the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a column containing True/False values using the `isnull()` function. Assign this column to the `Reviews_isnull` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, subset the `google_play` with `Reviews_isnull`. This should give you all the rows that contain non-numeric characters.\n",
    "\n",
    "Your output should look like:\n",
    "\n",
    "![Reviews_bool.png](reviews-bool.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We see that Google Play is using a shorthand for millions. \n",
    "\n",
    "Let's write a function to transform this data.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Create a function that returns the correct numeric values of *Reviews*.\n",
    "1. Define a test string with `M` in the last character.\n",
    "1. Test your function with the test string. Make sure your function works correctly. If not, modify your functions and test again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "def convert_string_to_numeric(s):\n",
    "    \"\"\"\n",
    "    Convert a string value to numeric. If the last character of the string is `M`, obtain the \n",
    "    numeric part of the string, multiply it with 1,000,000, then return the result. Otherwise, \n",
    "    convert the string to numeric value and return the result.\n",
    "    \n",
    "    Args:\n",
    "        s: The Reviews score in string format.\n",
    "\n",
    "    Returns:\n",
    "        The correct numeric value of the Reviews score.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to apply the function to the `Reviews` column in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the non-numeric `Reviews` row again. It should have been fixed now and you should see:\n",
    "\n",
    "![Reviews_bool_fixed.png](reviews-bool-fixed.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also check the variable types of `google_play`. The `Reviews` column should be a `float64` type now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next column we will look at is `Size`. We start by looking at all unique values in `Size`:\n",
    "\n",
    "*Hint: use `unique()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unique.html))*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have seen lots of unique values of the app sizes.\n",
    "\n",
    "#### While we can convert most of the `Size` values to numeric in the same way we converted the `Reviews` values, there is one value that is impossible to convert.\n",
    "\n",
    "What is that badass value? Enter it in the next cell and calculate the proportion of its occurence to the total number of records of `google_play`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### While this column may be useful for other types of analysis, we opt to drop it from our dataset. \n",
    "\n",
    "There are two reasons. First, the majority of the data are ordinal but a sizeable proportion are missing because we cannot convert them to numerical values. Ordinal data are both numerical and categorical, and they usually can be ranked (e.g. 82k is smaller than 91M). In contrast, non-ordinal categorical data such as blood type and eye color cannot be ranked. The second reason is as a categorical column, it has too many unique values to produce meaningful insights. Therefore, in our case the simplest strategy would be to drop the column.\n",
    "\n",
    "Drop the column in the cell below (use `inplace=True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's look at how many missing values are in each column. \n",
    "\n",
    "This will give us an idea of whether we should come up with a missing data strategy or give up on the column all together. In the next column, find the number of missing values in each column: \n",
    "\n",
    "*Hint: use the `isna()` and `sum()` functions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find the column with the most missing values is now `Rating`.\n",
    "\n",
    "#### What is the proportion of the missing values in `Rating` to the total number of records?\n",
    "\n",
    "Enter your answer in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sizeable proportion of the `Rating` column is missing. A few other columns also contain several missing values.\n",
    "\n",
    "#### We opt to preserve these columns and remove the rows containing missing data.\n",
    "\n",
    "In particular, we don't want to drop the `Rating` column because:\n",
    "\n",
    "* It is one of the most important columns in our dataset. \n",
    "\n",
    "* Since the dataset is not a time series, the loss of these rows will not have a negative impact on our ability to analyze the data. It will, however, cause us to lose some meaningful observations. But the loss is limited compared to the gain we receive by preserving these columns.\n",
    "\n",
    "In the cell below, remove all rows containing at least one missing value. Use the `dropna()` function ([documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html)). Assign the new dataframe to the variable `google_missing_removed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, we use the `google_missing_removed` variable instead of `google_play`.\n",
    "\n",
    "#### Next, we look at the `Last Updated` column.\n",
    "\n",
    "The `Last Updated` column seems to contain a date, though it is classified as an object type. Let's convert this column using the `pd.to_datetime` function ([documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The last column we will transform is `Price`. \n",
    "\n",
    "We start by looking at the unique values of this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all prices are ordinal data without exceptions, we can tranform this column by removing the dollar sign and converting to numeric. We can create a new column called `Price Numerical` and drop the original column.\n",
    "\n",
    "We will achieve our goal in three steps. Follow the instructions of each step below.\n",
    "\n",
    "#### First we remove the dollar sign. Do this in the next cell by applying the `str.replace` function to the column to replace `$` with an empty string (`''`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second step, coerce the `Price Numerical` column to numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, drop the original `Price` column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the variable types of `google_missing_removed`. Make sure:\n",
    "\n",
    "* `Size` and `Price` columns have been removed.\n",
    "* `Rating`, `Reviews`, and `Price Numerical` have the type of `float64`.\n",
    "* `Last Updated` has the type of `datetime64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2 - Loading and Extracting Features from the Second Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the second dataset to the variable `google_reviews`. The data is in the file `googleplaystore_user_reviews.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This dataset contains the top 100 reviews for each app. \n",
    "\n",
    "Let's examine this dataset using the `head` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The main piece of information we would like to extract from this dataset is the proportion of positive reviews of each app. \n",
    "\n",
    "Columns like `Sentiment_Polarity` and `Sentiment_Subjectivity` are not to our interests because we have no clue how to use them. We do not care about `Translated_Review` because natural language processing is too complex for us at present (in fact the `Sentiment`, `Sentiment_Polarity`, and `Sentiment_Subjectivity` columns are derived from `Translated_Review` the data scientists). \n",
    "\n",
    "What we care about in this challenge is `Sentiment`. To be more precise, we care about **what is the proportion of *Positive* sentiment of each app**. This will require us to aggregate the `Sentiment` data by `App` in order to calculate the proportions.\n",
    "\n",
    "Now that you are clear about what we are trying to achieve, follow the steps below that will walk you through towards our goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our first step will be to remove all rows with missing sentiment. \n",
    "\n",
    "In the next cell, drop all rows with missing data using the `dropna()` function and assign this new dataframe to `review_missing_removed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, use the `value_counts()` function ([documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html)) to get a sense on how many apps are in this dataset and their review counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the tough part comes. Let's plan how we will achieve our goal:\n",
    "\n",
    "1. We will count the number of reviews that contain *Positive* in the `Sentiment` column.\n",
    "\n",
    "1. We will create a new dataframe to contain the `App` name, the number of positive reviews, and the total number of reviews of each app.\n",
    "\n",
    "1. We will then loop the new dataframe to calculate the postivie review portion of each app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Count the number of positive reviews.\n",
    "\n",
    "In the following cell, write a function that takes a column and returns the number of times *Positive* appears in the column. \n",
    "\n",
    "*Hint: One option is to use the `np.where()` function ([documentation](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.where.html)).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code below\n",
    "\n",
    "def positive_function(x):\n",
    "    \"\"\"\n",
    "    Count how many times the string `Positive` appears in a column (exact string match).\n",
    "    \n",
    "    Args:\n",
    "        x: data column\n",
    "    \n",
    "    Returns:\n",
    "        The number of occurrences of `Positive` in the column data.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Create a new dataframe to contain the `App` name, the number of positive reviews, and the total number of reviews of each app\n",
    "\n",
    "We will group `review_missing_removed` by the `App` column, then aggregate the grouped dataframe on the number of positive reviews and the total review counts of each app. The result will be assigned to a new variable `google_agg`. Here is the ([documentation on how to achieve it](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.DataFrameGroupBy.agg.html)). Take a moment or two to read the documentation and google examples because it is pretty complex.\n",
    "\n",
    "When you obtain `google_agg`, check its values to make sure it has an `App` column as its index as well as a `Positive` column and a `Total` column. Your output should look like:\n",
    "\n",
    "![Positive Reviews Agg](positive-review-agg.png)\n",
    "\n",
    "*Hint: Use `positive_function` you created earlier as part of the param passed to the `agg()` function in order to aggregate the number of positive reviews.*\n",
    "\n",
    "#### Bonus:\n",
    "\n",
    "As of Pandas v0.23.4, you may opt to supply an array or an object to `agg()`. If you use the array param, you'll need to rename the columns so that their names are `Positive` and `Total`. Using the object param will allow you to create the aggregated columns with the desirable names without renaming them. However, you will probably encounter a warning indicating supplying an object to `agg()` will become outdated. It's up to you which way you will use. Try both ways out. Any way is fine as long as it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first 5 rows of `google_agg` to check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a derived column to `google_agg` that is the ratio of the `Positive` and the `Total` columns. Call this column `Positive Ratio`. \n",
    "\n",
    "Make sure to account for the case where the denominator is zero using the `np.where()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now drop the `Positive` and `Total` columns. Do this with `inplace=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first 5 rows of `google_agg`. Your output should look like:\n",
    "\n",
    "![Positive Reviews Agg](positive-review-ratio.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3 - Join the Dataframes\n",
    "\n",
    "In this part of the lab, we will join the two dataframes and obtain a dataframe that contains features we can use in our ML algorithm.\n",
    "\n",
    "In the next cell, join the `google_missing_removed` dataframe with the `google_agg` dataframe on the `App` column. Assign this dataframe to the variable `google`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at the final result using the `head()` function. Your final product should look like:\n",
    "\n",
    "![Final Product](google-final-head.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
